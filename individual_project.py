# -*- coding: utf-8 -*-
"""individual_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/155EPECkIgbnRR-lR0283ICnfyum7JnBS

We will build a binary classifier as well as a multiclassifier using the mnist dataset

let's import the relevant modules
"""

import numpy as np
import pandas as pd
from sklearn.datasets import fetch_openml

import matplotlib as mpl
import matplotlib.pyplot as plt

"""let's fetch our data from open ml"""

mnist=fetch_openml("mnist_784",version=1)

#let's check the datatype of mnist

type(mnist)

#bunch is dictionary like, let's look at the keys

mnist.keys()

"""split the dataset into features and target"""

features , target = mnist['data'],mnist['target']

#let's look at the type and shape of the dataset

print(type(features),features.shape)

print(type(target),(target.shape))

"""We have a dataframe now"""

#let's look at the head of the data

features.head()

"""Visualize one row"""

an_image=features.iloc[0,:]
an_image=np.array(an_image).reshape(28,28)#we get a 2d image with 28 by 28 res

plt.imshow(an_image,cmap=mpl.cm.binary,interpolation='nearest');
plt.axis('off')

"""What does this image corresponds to"""

output=target.iloc[0]
output

"""the image corresponds to a 5, and it is a string

change the labels back to int
"""

target=target.astype(np.uint8)#int8 is enough to rep numbers from 0 to 9

"""let's check for missing values"""

features.isnull().sum()

"""There are no missing values in the features

check for missing values in the target set
"""

target.isnull().sum()

"""no missing values in target set

Let's prepare the data

NB: The first 60000 of the mnist dataset is for training while the last 10000 is for testing.

there are no outliers, categorical values or missing values so let's start splitting
"""

train_features, train_targets= features[:60000],target[:60000]

test_features, test_targets= features[60000:],target[60000:]

"""Let's check the shape of our datasets"""

print(test_features.shape, test_targets.shape)

print(train_features.shape, train_targets.shape)

"""Training a model

Binary Classifier: Model to detect 5 from any other number
"""

#let's create the labels
train_label_5=(train_targets==5)#True for all 5s false for all other value

test_label_5=(test_targets==5)

"""pick an sgdclassifier and train it on the whole training set"""

from sklearn.linear_model import SGDClassifier

clf=SGDClassifier(random_state=42)

clf.fit(train_features,train_label_5)

"""let's make a prediction"""

a_digit=features.iloc[0,:].values
a_digit_res=a_digit.reshape(1,-1)
a_digit_res.shape

clf.predict(a_digit_res)

"""k-fold Cross validation with k=3"""

from sklearn.model_selection import cross_val_score
cross_val_score(clf,train_features,train_label_5,cv=3,scoring='accuracy')

"""The accuracy is very high

let's check for the total number of 5's in the dataset
"""

five_s=train_label_5.value_counts()[True]

(five_s/train_targets.shape[0])*100

"""The data is inbalance hence Accuracy is not a good metric for measuring the performance of the model

Check confusion matrix
"""

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_predict

predict=cross_val_predict(clf,train_features,train_label_5,cv=3)

cf_matrix=confusion_matrix(train_label_5,predict)

cf_matrix

"""Now let's calculate the precision and recall score"""

from sklearn.metrics import precision_score, recall_score

pre_score=precision_score(train_label_5,predict)

re_score=recall_score(train_label_5,predict)

print(pre_score,re_score)

"""precion of 83.7% and a recall of 65.1%

of the total amount of 5's the model correctly predicted just 65% as 5

The precision is telling us that the model is correct 83.7% of the time

let's look at the decision function
"""

scores=clf.decision_function(a_digit_res)
scores

"""Recall and precision can be controlled by controlling the descision score

Evaluation
"""

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
#scale test set
test_features_scaled=scaler.fit_transform(test_features.astype(np.float64))

# make predictions for the test set

test_predict=cross_val_predict(clf,test_features_scaled,test_targets,cv=3)

from sklearn.metrics import classification_report

print(classification_report(test_targets,test_predict))

"""NOw let's create multiclass classifier using ovA"""

mult_clf=SGDClassifier(random_state=42)

mult_clf.fit(train_features,train_targets)

#let's make a single prediction

mult_clf.predict(a_digit_res)

"""it got it right

let's check the cross val score
"""

cross_val_score(mult_clf,train_features,train_targets,cv=3)

"""That's 87% accuracy not bad

Accuracy can be improved by scaling the input
"""

#A look at the confusion_matrix

predictions=cross_val_predict(mult_clf,train_features,train_targets,cv=3)
conf_mat=confusion_matrix(train_targets,predictions)

plt.matshow(conf_mat,cmap=plt.cm.gray)

""" print the classification report"""

target_names=['Class 0','Class 1','Class 2','Class 3','Class 4','Class 5','Class 6','Class 7','Class 8','Class 9',]

print(classification_report(train_targets,predictions,target_names=target_names))

"""Let's improve the model by increasing the train data using shifted images"""

#let's increase the training dataset

from scipy.ndimage import shift
train_features_shifted_1=shift(train_features,shift=[0,1])

train_features_shifted_2=shift(train_features,shift=[0,-1])

train_features_shifted_3=shift(train_features,shift=[1,0])

train_features_shifted_4=shift(train_features,shift=[-1,0])

train_features_shifted_5=shift(train_features,shift=1)

"""let's put all the features included the shifted features together"""

train_features_array=np.array(train_features)

all_features=[train_features,train_features_shifted_1,train_features_shifted_2,train_features_shifted_3,train_features_shifted_4,train_features_shifted_5]

type(all_features)

"""The result is a list let's convert it into an array"""

all_features=np.array(all_features).reshape(360000,784)

"""Let's check the shape of all features"""

all_features.shape

type(train_targets)

all_targets=[train_targets,train_targets,train_targets,train_targets,train_targets,train_targets]

all_targets=np.array(all_targets).reshape(360000,)

all_targets.shape

"""Train the sgdclassifier on the larger dataset"""

sgd_clf_large=SGDClassifier(random_state=42)

#train the classifier

sgd_clf_large.fit(all_features,all_targets)

# scale test set
test_features_scaled = scaler.fit_transform(test_features.astype(np.float64))

# make predictions for the test set
test_pred = cross_val_predict(sgd_clf_large, test_features_scaled, test_targets, cv=3)

# evaluate
print(classification_report(test_targets, test_pred))

